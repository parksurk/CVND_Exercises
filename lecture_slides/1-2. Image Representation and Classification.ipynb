{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 1-2 : Image Representation and Classification\n",
    "- See how images are represented numerically.\n",
    "- Implement image processing techniques like color masking and binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-1 : What is Image Representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Image Classification](./images/1-2-1-1_Image_Classification.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Semantic Gap](./images/1-2-1-2_Semantic_Gap.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Challenges - Viewpoint Variation](./images/1-2-1-3_Challenges_Viewpoint_Variation.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Challenges - illumination](./images/1-2-1-4_Challenges_illumination.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Challenges - Deformation](./images/1-2-1-5_Challenges_Deformation.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Challenges - Occlusion](./images/1-2-1-6_Challenges_Occlusion.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Challenges - Background Cutter](./images/1-2-1-7_Challenges_Background_Cutter.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Challenges - Intraclass variation](./images/1-2-1-8_Challenges_Intraclass_variation.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Image Classifier](./images/1-2-1-9_Image_Classifier.png)\n",
    "- image ref : CS231n by Stanford Univ. Fei-Fei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-2 : Cognitive Intelligence vs. Emotional Intelligence\n",
    "\n",
    "- Cognitive intelligence is the ability to reason and understand the world based on observations and facts. It's often what is measured on academic tests and what's measured to calculate a person's IQ.\n",
    "\n",
    "- Emotional intelligence is the ability to understand and influence human emotion. For example, observing that someone looks sad based on their facial expression, body language, and what you know about them - then acting to comfort them or asking them if they want to talk, etc. For humans, this kind of intelligence allows us to form meaningful connections and build a trustworthy network of friends and family. It's also often thought of as only a human quality and is not yet a part of traditional AI systems.\n",
    "\n",
    "- ref : Affectiva and emotion AI, check out http://www.affectiva.com/\n",
    "- Emotional Intelligence demo : https://demo.mr.affectiva.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-3 : Computer Vision Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General computer vision processing pipeline\n",
    "\n",
    "![General computer vision processing pipeline](./images/1-2-3-1_General_computer_vision_processing_pipeline.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Facial recognition pipeline.\n",
    "\n",
    "![Facial recognition pipeline](./images/1-2-3-2_Facial_recognition_pipeline.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Standardizing Data\n",
    "\n",
    "Pre-processing images is all about standardizing input images so that you can move further along the pipeline and analyze images in the same way. In machine learning tasks, the pre-processing step is often one of the most important.\n",
    "\n",
    "![Standardizing Data](./images/1-2-3-3_Standardizing_Data.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Imagae Classification pipeline in practice](./images/1-2-3-4_Imagae_Classification_pipeline_in_practice.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-4 : Images as Numerical Data\n",
    "\n",
    "Every pixel in an image is just a numerical value and, we can also change these pixel values. We can multiply every single one by a scalar to change how bright the image is, we can shift each pixel value to the right, and many more operations!\n",
    "\n",
    "### Treating images as grids of numbers is the basis for many image processing techniques.\n",
    "\n",
    "Most color and shape transformations are done just by mathematically operating on an image and changing it pixel-by-pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![grayscale image of a car](./images/1-2-4-1_grayscale_image_of_a_car.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Images as Grids of Pixels](./images/1-2-4-2_Images_as_Grids_of_Pixels.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Pixel Value](./images/1-2-4-3_Pixel_Value.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Pixel Location](./images/1-2-4-4_Pixel_Location.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/1. Images as Numerical Data.ipynb\">1. Images as Numerical Data.ipynb</a> \n",
    "\n",
    "![Day Night](./images/1-2-4-5_Day_Night.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-5 : Color Images\n",
    "Color images are interpreted as 3D cubes of values with width, height, and depth!\n",
    "\n",
    "The depth is the number of colors. Most color images can be represented by combinations of only 3 colors: red, green, and blue values; these are known as RGB images. And for RGB images, the depth is 3!\n",
    "\n",
    "It’s helpful to think of the depth as three stacked, 2D color layers. One layer is Red, one Green, and one Blue. Together they create a complete color image.\n",
    "\n",
    "![RGB layers of a car image](./images/1-2-5-1_RGB_layers_of_a_car_image.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Importance of Color\n",
    "In general, when you think of a classification challenge, like identifying lane lines or cars or people, you can decide whether color information and color images are useful by thinking about your own vision.\n",
    "\n",
    "If the identification problem is easier in color for us humans, it’s likely easier for an algorithm to see color images too!\n",
    "\n",
    "![Importance_of_Color](./images/1-2-5-2_Importance_of_Color.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/2. Visualizing RGB Channels.ipynb\">2. Visualizing RGB Channels.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1-2-6 : Color Thresholds for Selecting areas of interest\n",
    "Color thresholds are used in a number of applications including extensively in computer graphics and video.\n",
    "A common use is with a blue screen.\n",
    "A blue screen similar to a green screen is used to layer two images or video streams based on identifying and replacing a large blue area.\n",
    "We're using a blue screen to film right now and the blue background can be replaced so that it looks like I'm in a different room,\n",
    "\n",
    "![blue screen in film](./images/1-2-6-1_blue_screen_in_film.jpg)\n",
    "- image ref : Scenes of Guardians of the Galaxy Vol. 2(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OpenCV\n",
    "OpenCV is a popular computer vision library that has meany built in tools for image analysis and understanding!\n",
    "\n",
    "#### Why BGR instead of RGB?\n",
    "OpenCV reads in images in BGR format (instead of RGB) because when OpenCV was first being developed, BGR color format was popular among camera manufacturers and image software providers. The red channel was considered one of the least important color channels, so was listed last, and many bitmaps use BGR format for image storage. However, now the standard has changed and most image software and cameras use RGB format, which is why, in these examples, it's good practice to initially convert BGR images to RGB before analyzing or manipulating them.\n",
    "\n",
    "#### Changing Color Spaces\n",
    "To change color spaces, we used OpenCV's cvtColor function, whose documentation is here http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/3. Blue Screen.ipynb\">3. Blue Screen.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/4. Green Screen Car.ipynb\">4. Green Screen Car.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-7 : Color Spaces and Transforms\n",
    "\n",
    "There are many other ways to represent the colors in an image besides just composed of red, green, and blue values.\n",
    "These different color representations are often called \"color spaces\".\n",
    "- RGB (red, green, blue)\n",
    "- HSV (hue, saturation, value) = (색상, 채도, 명도)\n",
    "- HLS (hue, lightness, saturation) = (색상, 명도, 채도)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![blue screen color selection](./images/1-2-7-1_blue_screen_color_selection.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![blue screen color selection fails](./images/1-2-7-2_blue_screen_color_selection_fails.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![color spaces](./images/1-2-7-3_color_spaces.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![color spaces_rgb hsv hls](./images/1-2-7-4_color_spaces_rgb_hsv_hls.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![color spaces_hsv](./images/1-2-7-5_color_spaces_hsv.png)\n",
    "\n",
    "1. 색상(Hue): 색상값 H는 가시광선 스펙트럼을 주파수 별로 고리모양으로 배치했을 때의 각도이다. 0°~360°의 범위를 갖고 360°와 0°는 빨강을 가리킨다.\n",
    "2. 채도(Saturation): 채도값 S는 특정한 색상의 진함의 정도를 나타낸다. 가장 진한 상태를 100%이고 0%는 같은 명도의 무채색이다.\n",
    "3. 명도(Value): 명도값 V는 밝은 정도를 나타낸다. 순수한 흰색, 빨간색은 100%이고 검은색은 0%이다.\n",
    "\n",
    "\n",
    "\n",
    "* image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Color Spaces Transformation\n",
    "\n",
    "- OpenCV ref : https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Color Selection\n",
    "To select the most accurate color boundaries, it's often useful to use a color picker(https://www.w3schools.com/colors/colors_picker.asp) and choose the color boundaries that define the region you want to select! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/5_1. HSV Color Space, Balloons.ipynb\">5_1. HSV Color Space, Balloons.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/5_2. Green screen, HSV conversion.ipynb\">5_2. Green screen, HSV conversion.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-8 : Visualizing the Data\n",
    "\n",
    "Before you can classify any set of images, you have to look at them! Visualizing the image data you’re working with is the first step in identifying any patterns in image data and being able to make predictions about the data.\n",
    "\n",
    "![Two images of the same scene](./images/1-2-8-1_Two_images_of_the_same_scene.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/6_1. Visualizing the Data.ipynb.ipynb\">6_1. Visualizing the Data.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-9 : Labeled Data and Accuracy\n",
    "\n",
    "### Why do we need labels?\n",
    "You can tell if an image is night or day, but a computer cannot unless we tell it explicitly with a label!\n",
    "\n",
    "This becomes especially important when we are testing the accuracy of a classification model.\n",
    "\n",
    "![what is label](./images/1-2-9-1_what_is_label.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Image can have multiple labels](./images/1-2-9-2_Image_can_have_multiple_labels.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![true_label predicted_label match](./images/1-2-9-3_true_label_predicted_label_match.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree\n",
    "\n",
    "A classifier takes in an image as input and should output a predicted_label that tells us the predicted class of that image. Now, when we load in data, like you’ve seen, we load in what are called the true_labels which are the correct labels for the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![true_label predicted_label not match](./images/1-2-9-4_true_label_predicted_label_not_match.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree\n",
    "\n",
    "To check the accuracy of a classification model, we compare the predicted and true labels. If the true and predicted labels match, then we’ve classified the image correctly! Sometimes the labels do not match, which means we’ve misclassified an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy\n",
    "After looking at many images, the accuracy of a classifier is defined as the number of correctly classified images (for which the predicted_label matches the true label) divided by the total number of images. So, say we tried to classify 100 images total, and we correctly classified 81 of them. We’d have 0.81 or 81% accuracy!\n",
    "\n",
    "![definition of accuracy](./images/1-2-9-5_definition_of_accuracy.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![learn from misclassified images](./images/1-2-9-6_learn_from_misclassified_images.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Numerical labels\n",
    "\n",
    "It’s good practice to use numerical labels instead of strings or categorical labels. They're easier to track and compare. So, for our day and night, binary class example, instead of \"day\" and \"night\" labels we’ll use the numerical labels: 0 for night and 1 for day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-10 : Feature\n",
    "\n",
    "### Feature = Distinguishing and Measurable Traits\n",
    "\n",
    "You may have thought about a number of distinguishing features: day images are much brighter, generally, than night images. Night images also have these really bright small spots, so the brightness over the whole image varies a lot more than the day images. There is a lot more of a gray/blue color palette in the day images.\n",
    "\n",
    "There are lots of measurable traits that distinguish these images, and these measurable traits are referred to as features.\n",
    "\n",
    "A feature a measurable component of an image or object that is, ideally, unique and recognizable under varying conditions - like under varying light or camera angle. And we’ll learn more about features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-11 : Standardizing Output\n",
    "\n",
    "### Numerical vs. Categorical\n",
    "Let's learn a little more about labels. After visualizing the image data, you'll have seen that each image has an attached label: \"day\" or \"night,\" and these are known as categorical values.\n",
    "\n",
    "Categorical values are typically text values that represent various traits about an image. A couple examples are:\n",
    "\n",
    "An \"animal\" variable with the values: \"cat,\" \"tiger,\" \"hippopotamus,\" and \"dog.\"\n",
    "A \"color\" variable with the values: \"red,\" \"green,\" and \"blue.\"\n",
    "Each value represents a different category, and most collected data is labeled in this way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These labels are descriptive for us, but may be inefficient for a classification task. Many machine learning algorithms do not use categorical data; they require that all output be numerical. Numbers are easily compared and stored in memory, and for this reason, we often have to convert categorical values into numerical labels. There are two main approaches that you'll come across:\n",
    "\n",
    "- Integer encoding\n",
    "- One hot-encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Integer Encoding\n",
    "Integer encoding means to assign each category value an integer value. So, day = 1 and night = 0. This is a nice way to separate binary data, and it's what we'll do for our day and night images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One-hot Encoding\n",
    "One-hot encoding is often used when there are more than 2 values to separate. A one-hot label is a 1D list that's the length of the number of classes. Say we are looking at the animal variable with the values: \"cat,\" \"tiger,\" \"hippopotamus,\" and \"dog.\" There are 4 classes in this category and so our one-hot labels will be a list of length four. The list will be all 0's and one 1; the 1 indicates which class a certain image is.\n",
    "\n",
    "For example, since we have four classes (cat, tiger, hippopotamus, and dog), we can make a list in that order: [cat value, tiger value, hippopotamus value, dog value]. In general, order does not matter.\n",
    "\n",
    "If we have an image and it's one-hot label is [0, 1, 0, 0], what does that indicate?\n",
    "\n",
    "In order of [cat value, tiger value, hippopotamus value, dog value], that label indicates that it's an image of a tiger! Let's do one more example, what about the label [0, 0, 0, 1]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/6_2. Standardizing the Data.ipynb\">6_2. Standardizing the Data.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-13 : Average Brightness\n",
    "\n",
    "Here were the steps we took to extract the average brightness of an image.\n",
    "\n",
    "1. Convert the image to HSV color space (the Value channel is an approximation for brightness)\n",
    "2. Sum up all the values of the pixels in the Value channel\n",
    "3. Divide that brightness sum by the area of the image, which is just the width times the height.\n",
    "\n",
    "This gave us one value: the average brightness or the average Value of that image.\n",
    "In the next notebook, make sure to look at a variety of day and night images and see if you can think of an average brightness value that will separate the images into their respective classes!\n",
    "\n",
    "The next step will be to feed this data into a classifier. A classifier might be as simple as a conditional statement that checks if the average brightness is above some threshold, then this image is labeled as 1 (day) and if not, it’s labeled as 0 (night).\n",
    "\n",
    "On your own, you can choose to create more features that help distinguish these images from one another, and we’ll soon learn about testing the accuracy of a model like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/6_3. Average Brightness.ipynb\">6_3. Average Brightness.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-14 : Classification Task\n",
    "Let’s now complete our day and night classifier. After we extracted the average brightness value, we want to turn this feature into a predicted_label that classifies the image. Remember, we want to generate a numerical label, and again, since we have a binary dataset, I’ll create a label that is a 1 if an image is predicted to be day and a 0 for images predicted to be night.\n",
    "\n",
    "I can create a complete classifier by writing a function that takes in an image, extracts the brightness feature, and then checks if the average brightness is above some threshold X.\n",
    "\n",
    "If it is, this classifier returns a 1 (day), and if it’s not, this classifier returns a 0 (night)!\n",
    "Next, you'll take a look at this notebook and get a chance to tweak the threshold parameter. Then, when you're able to generate predicted labels, you can compare them to the true labels, and check the accuracy of our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/6_4. Classification.ipynb\">6_4. Classification.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-2-15 : Evaluation Metrics\n",
    "\n",
    "### Accuracy\n",
    "The accuracy of a classification model is found by comparing predicted and true labels. For any given image, if the predicted_label matches thetrue_label, then this is a correctly classified image, if not, it is misclassified.\n",
    "\n",
    "The accuracy is given by the number of correctly classified images divided by the total number of images. We’ll test this classification model on new images, this is called a test set of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test Data\n",
    "Test data is previously unseen image data. The data you have seen, and that you used to help build a classifier is called training data, which we've been referring to. The idea in creating these two sets is to have one set that you can analyze and learn from (training), and one that you can get a sense of how your classifier might work in a real-world, general scenario. You could imagine going through each image in the training set and creating a classifier that can classify all of these training images correctly, but, you actually want to build a classifier that recognizes general patterns in data, so that when it is faced with a real-world scenario, it will still work!\n",
    "\n",
    "So, we use a new, test set of data to see how a classification model might work in the real-world and to determine the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Misclassified Images\n",
    "In this and most classification examples, there are a few misclassified images in the test set. To see how to improve, it’s useful to take a look at these misclassified images; look at what they were mistakenly labeled as and where your model fails. It will be up to you to look at these images and think about how to improve the classification model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_1_Image_Representation/6_5. Accuracy and Misclassification.ipynb\">6_5. Accuracy and Misclassification.ipynb</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cv-nd",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
