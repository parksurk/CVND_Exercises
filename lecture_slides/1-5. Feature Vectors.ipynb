{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 1-5 : Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-1 : Corners and Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Features alone and together\n",
    "\n",
    "![Corner Detection on an image](./images/1-5-1-1_Corner_Detection_on_an_image.png)\n",
    "\n",
    "Now, you've seen examples of shape-based features, like corners, that can be extracted from images, but how can we actually uses the features to detect whole objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to create Feature Vectors that can then be used to recognize different objects.\n",
    "\n",
    "Say we want a way to detect this mountain in other images, too. A single corner will not be enough to identify this mountain in any other images, but, we can take a set of features that define the shape of this mountain, group them together into an array or vector, and then use that set of features to create a mountain detector!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corners are Gradient Features\n",
    "\n",
    "![Corners are Gradient Features](./images/1-5-1-2_Corners_are_Gradient_Features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We have to look at distinct sets of features often called \"feature vectors\".\n",
    "\n",
    "![distinct sets of features](./images/1-5-1-3_distinct_sets_of_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We use patterns in image gradients to recognize different objects.\n",
    "\n",
    "![patterns in image gradients](./images/1-5-1-4_patterns_in_image_gradients.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-2 : Real-Time Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Having a fast Object Recognition Algorithm is essential for many computer vision applications\n",
    "\n",
    "![RTFD in CV application Self-driving Car](./images/1-5-2-1_RTFD_in_CV_application_Self-driving_Car.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ORB algorithm is used to quickly create feature vectors of key points in an image\n",
    "\n",
    "![RTFD in CV application AR](./images/1-5-2-2_RTFD_in_CV_application_AR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-3 : Introduction to ORB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ORB = Oriented FAST and Rotated BRIEF\n",
    "\n",
    "![ORB](./images/1-5-3-1_ORB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keypoint\n",
    "\n",
    "![Keypoint](./images/1-5-3-2_Keypoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Keypoint and Corners](./images/1-5-3-3_Keypoint_and_Corners.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Keypoint and Binary Feature Vectors](./images/1-5-3-4_Keypoint_and_Binary_Feature_Vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-4 : FAST\n",
    "\n",
    "The first step in ORB feature detection is to find the key points in an image which is done by the FAST Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FAST = Features from Accelerated Segments Test\n",
    "\n",
    "![FAST](./images/1-5-4-1_FAST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Intensity of Pixel](./images/1-5-4-2_IP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![classifiying 16 pixcels into 3 classes](./images/1-5-4-3_classifiying_16_pixcels_into_3_classes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![only four equidistant pixels in the circle](./images/1-5-4-4_only_four_equidistant_pixels_in_the_circle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Keypoints found by FAST](./images/1-5-4-5_Keypoints_found_by_FAST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Location of object defining edges in an image](./images/1-5-4-6_Location_of_object_defining_edges_in_an_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### However, one thing to note is that these key points only give us the location of an edge, and don't include any information about the direction of the change of intensity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Quiz : FAST Keypoints]\n",
    "\n",
    "Zoomed in patch around an arch in a building. Original image taken from OpenCV's documentation.\n",
    "\n",
    "![FAST keypoints Quiz](./images/1-5-4-7_FAST_keypoints_quiz.png)\n",
    "\n",
    "[QUIZ QUESTION]Do you think the pixel, p, above, will be identified as a keypoint by the FAST algorithm?\n",
    "- Yes\n",
    "- No\n",
    "- Not enough information to tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-5 : BRIEF\n",
    "\n",
    "The second step of the orb algorithm is to take the key points found by the first algorithm and turn those into feature vectors that together can represent an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BRIEF = Binary Robust Independent Elementary Features\n",
    "\n",
    "![BRIEF](./images/1-5-5-1_BRIEF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BRIEF creates Binary Feature Vectors from a set of Keypoints\n",
    "\n",
    "![creates Binary Feature Vectors from a set of Keypoints](./images/1-5-5-2_creates_Binary_Feature_Vectors_from_a_set_of_Keypoints.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Binary Feature Vector = Binary Descriptor\n",
    "\n",
    "![Binary Feature Vector](./images/1-5-5-3_Binary_Feature_Vector.png)\n",
    "\n",
    "Computers run on binary or machine code, and so the great advantage of using binary feature vectors is that they can be stored very efficiently in memory and they can be computed quickly.\n",
    "These properties not only make BRIEF very fast, which is crucial for real time applications, but they also allow BRIEF to run on devices with very limited computational resources, such as your smartphone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Original Image\n",
    "\n",
    "![BRIEF step0](./images/1-5-5-4_BRIEF_step0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1 : Smoothing image\n",
    "\n",
    "![BRIEF step1](./images/1-5-5-5_BRIEF_step1.png)\n",
    "\n",
    "The BRIEF algorithm starts by smoothing a given image with a Gaussian kernel in order to prevent the descriptor from being too sensitive to high frequency noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2 : Selecting a random pair of pixels\n",
    "\n",
    "![BRIEF step2](./images/1-5-5-6_BRIEF_step2.png)\n",
    "\n",
    "- First pixel is from Gaussian distribution centered around the key point and with a standard deviation or a spread of Sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step2-1](./images/1-5-5-7_BRIEF_step2-1.png)\n",
    "\n",
    "- Second pixel is from a Gaussian distribution centered around that first pixel and with a standard deviation of Sigma over 2\n",
    "- These choice of Gaussians improves the feature matching rates empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3 : Constructing the binary descriptor\n",
    "\n",
    "![BRIEF step3](./images/1-5-5-8_BRIEF_step3.png)\n",
    "\n",
    "- If the first pixel is brighter than the second, it assigns the value of one to the corresponding bit in the descriptor.\n",
    "- Otherwise it assigns the value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-1](./images/1-5-5-9_BRIEF_step3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-2](./images/1-5-5-10_BRIEF_step3-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-3](./images/1-5-5-11_BRIEF_step3-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-4](./images/1-5-5-12_BRIEF_step3-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-5](./images/1-5-5-13_BRIEF_step3-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-6](./images/1-5-5-14_BRIEF_step3-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-7](./images/1-5-5-15_BRIEF_step3-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BRIEF step3-8](./images/1-5-5-16_BRIEF_step3-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-6 : Scale and Rotation Invariance\n",
    "\n",
    "ORB uses fast to detect keypoints in an image. And it goes to a couple of extra steps to make sure that it can detect objects no matter their size, or location in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Original Image\n",
    "\n",
    "![Scale and Rotation Invariance step0](./images/1-5-6-1_Scale_and_Rotation_Invariance_step0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1 : Building an image pyramid\n",
    "\n",
    "![Scale and Rotation Invariance step1](./images/1-5-6-2_Scale_and_Rotation_Invariance_step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2 : Locating key points at each level\n",
    "\n",
    "![Scale and Rotation Invariance step2](./images/1-5-6-3_Scale_and_Rotation_Invariance_step2.png)\n",
    "\n",
    "In this way ORB is partially scale invariant.\n",
    "This is of great importance because objects are unlikely to appear at the exact same size in every image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3 : Assigning an orientation to each keypoint\n",
    "\n",
    "![Scale and Rotation Invariance step3](./images/1-5-6-4_Scale_and_Rotation_Invariance_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4 : Repeating the same process for the images at all the other pyramid levels\n",
    "\n",
    "![Scale and Rotation Invariance step4](./images/1-5-6-5_Scale_and_Rotation_Invariance_step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In this way ORB algorithm is rotation invariant\n",
    "\n",
    "![Scale and Rotation Invariance final](./images/1-5-6-6_Scale_and_Rotation_Invariance_final.png)\n",
    "\n",
    "It's important to note that the patch size is not reduced in size at each level of the image pyramid.\n",
    "Therefore, the image area covered by the same patch at each level of the pyramid will be larger.\n",
    "This results in key points having different sizes. Which can be seen here.\n",
    "\n",
    "In this image the circles represent the size of each key point.\n",
    "Key points with the bigger size were found in higher levels of the pyramid.\n",
    "After having located and assigned an orientation to the key points, ORB now uses a modified version of BRIEF to create the feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_4_Feature_Vectors/1. Image Pyramids.ipynb\">1. Image Pyramids.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-7 : Feature Matching\n",
    "\n",
    "We use ORB descriptors to perform object recognition.\n",
    "ORB can detect the same object at different scales and orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training image & Query image\n",
    "\n",
    "![Training image and Query image](./images/1-5-7-1_Feature_Matching_step0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1 : Calculate the ORB descriptor for the training image and save it in memory\n",
    "\n",
    "![Calculate the ORB descriptor for the training image](./images/1-5-7-2_Feature_Matching_step1.png)\n",
    "\n",
    "The ORB descriptor will contain the binary feature vectors that describe each key point in this training image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2 : Compute and save the ORB descriptor for the query image\n",
    "\n",
    "![Compute and save the ORB descriptor for the query image](./images/1-5-7-3_Feature_Matching_step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3 : Perform key point matching between the two images\n",
    "\n",
    "![Compute and save the ORB descriptor for the query image](./images/1-5-7-4_Feature_Matching_step3.png)\n",
    "\n",
    "This matching is usually performed by a matching function.\n",
    "It's important to keep in mind that different matching functions will have different metrics for determining the quality of the match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ORB uses Hamming Metric\n",
    "\n",
    "For binary descriptors like the ones used by ORB the Hamming Metric is usually used because it can be performed extremely fast.\n",
    "\n",
    "The hamming metric determines the quality of the match between two key points by counting the number of disimilar bits between their binary descriptors.\n",
    "\n",
    "When comparing the key points in the training image with the ones in the query image, the pair with the smallest number of differences is considered to be the best match.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4 : Display best matching points between our training image and our query image\n",
    "\n",
    "![Display best matching points](./images/1-5-7-5_Feature_Matching_step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ORB in Video\n",
    "\n",
    "One common use for ORB, is in tracking and identifying objects in real time video streams.\n",
    "In this case, we compute the ORB descriptors for any images or objects we want to detect, before seeing a video stream and save those descriptors.\n",
    "Then, for each frame in an incoming video stream, we calculate ORB descriptors and use a matching function to compare the key points in the current video frame with the saved descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ORB in Video](./images/1-5-7-6_ORB_in_Video_example1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ORB in Video](./images/1-5-7-7_ORB_in_Video_example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ORB in Video](./images/1-5-7-8_ORB_in_Video_example3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Match Threshold\n",
    "\n",
    "The Metch Threshold is a free parameter that you can set.\n",
    "\n",
    "For example, if the ORB descriptor for a particular object has 100 key points, then you could set the threshold to be 35 percent, 50 percent, or 90 percent of the number of key points for that particular object.\n",
    "\n",
    "If you set the threshold to 35 percent, then that means that at least 35 key points out of the 100 that describe that object, must match in order to say that the object is in the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The ORB algorithm works best when you want to detect objects that have a lot of consistent features that are not affected by the background of an image.\n",
    "\n",
    "For example, ORB works well for facial detection, because faces have a lot of features such as the corner of the eyes and the mouth, that don't appear to change no matter where a person is. These features are consistent from image to image.\n",
    "\n",
    "However, ORB does not work so well when attempting to do more general object recognition.\n",
    "Say pedestrian detection in images,in which the shape and features of a person's body vary depending on clothing and movement.\n",
    "\n",
    "For this type of general object recognition, other algorithms like 'HOG algoritm' work much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_4_Feature_Vectors/2. ORB.ipynb\">2. ORB.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-5-8 : HOG\n",
    "\n",
    "In computer vision there are many algorithms that are designed to extract spatial features and identify objects using information about image gradients.\n",
    "One illustrative technique is called HOG or Histogram of Oriented Gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Feature Extraction\n",
    "\n",
    "![Gradient Feature Extraction](./images/1-5-8-1_Gradient_Feature_Extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HOG = Histogram of Oriented Gradients\n",
    "\n",
    "![Histogram of Oriented Gradients](./images/1-5-8-2_Histogram_of_Oriented_Gradients.png)\n",
    "\n",
    "A histogram is a graphical representation of the distribution of data.\n",
    "It looks a bit like a bar graph with bars of different heights.\n",
    "Each bar represents a group of data that falls in a certain range of values, also called bins, and taller bars indicate that more data falls into a certain bin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![HOG example1](./images/1-5-8-3_HOG_example1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![HOG example2](./images/1-5-8-4_HOG_example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![HOG produce histogram of gradient direction](./images/1-5-8-5_HOG_produce_histogram_of_gradient_direction.png)\n",
    "\n",
    "Oriented just means the direction or orientation of an image gradient.\n",
    "And we've already discussed how both the magnitude and the direction of a gradient can be calculated using an sobel operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1 : Calculate the magnitude and direction of the gradient at each pixel\n",
    "\n",
    "![Calculate the magnitude and direction of the gradient at each pixel](./images/1-5-8-6_HOG_produce_histogram_step1.png)\n",
    "\n",
    "This can be a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2 : Group these pixels into square cells\n",
    "\n",
    "![Group these pixels into square cells](./images/1-5-8-7_HOG_produce_histogram_step2.png)\n",
    "\n",
    "It actually groups these pixels into larger square cells typically eight by eight or smaller grids for smaller pictures.\n",
    "For the eight by eight case, it will have 64 gradient values, then for each of these cells it counts up how many of these gradients are in a certain direction and sums the magnitude of these gradients so that the strength of the gradients are accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3 : Count how many gradients in each cell fall in a certain range of orientations\n",
    "\n",
    "![Count how many gradients in each cell](./images/1-5-8-8_HOG_produce_histogram_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4 : Use these HOG features to train a classifier\n",
    "\n",
    "\n",
    "This histogram of oriented gradients is actually a feature vector.\n",
    "The next step will be to actually use these HOG features to train a classifier.\n",
    "The idea is that among images of the same object at different scales and orientations, the same pattern of HOG features can be used to detect the object wherever and however it appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_4_Feature_Vectors/3_1. HOG.ipynb\">3_1. HOG.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_4_Feature_Vectors/3_2. HOG Examples.ipynb\">3_2. HOG Examples.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Histograms in OpenCV\n",
    "Histograms are used in a variety of image analysis techniques and in creating feature vectors. Learn more about how to use histograms in OpenCV, here( http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_table_of_contents_histograms/py_table_of_contents_histograms.html )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning to Find Features\n",
    "\n",
    "So far, You've seen a number of feature extraction techniques, you should have a good understanding of how different objects and areas in an image can be identified by their unique shapes and colors.\n",
    "\n",
    "Convolutional filters and ORB and HOG descriptors all rely on patterns of intensity to identify different shapes (like edges) and eventually whole objects (with feature vectors). You've even seen how k-means clustering can be used to group data without any labels.\n",
    "\n",
    "Next, we'll see how to define and train a Convolutional Neural Network (CNN) that learns to extract important features from images."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cv-nd",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
