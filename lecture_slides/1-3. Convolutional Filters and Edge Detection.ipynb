{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 1-3 : Convolutional Filters and Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-1 : Filters and Finding Edges\n",
    "\n",
    "![what is filter](./images/1-3-1-1_what_is_filter.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree\n",
    "\n",
    "we’ve seen how to use color to help isolate a desired portion of an image and even help classify an image!\n",
    "\n",
    "In addition to taking advantage of color information, we also have knowledge about patterns of grayscale intensity in an image. Intensity is a measure of light and dark similar to brightness, and we can use this knowledge to detect other areas or objects of interest. For example, you can often identify the edges of an object by looking at an abrupt change in intensity, which happens when an image changes from a very dark to light area, or vice versa.\n",
    "To detect these changes, you’ll be using and creating specific image filters that look at groups of pixels and detect big changes in intensity in an image. These filters produce an output that shows these edges.\n",
    "\n",
    "So, let’s take a closer look at these filters and see when they’re useful in processing images and identifying traits of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-2 : Frequency in images\n",
    "\n",
    "![frequency in sound wave](./images/1-3-2-1_frequency_in_sound_wave.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree\n",
    "\n",
    "We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s (Hz), and high pitches and made by high-frequency waves. Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound and on the x-axis is time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### High and low frequency in image patterns\n",
    "\n",
    "![High and low frequency in image patterns](./images/1-3-2-2_High_and_low_frequency_in_image_patterns.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree\n",
    "\n",
    "Similarly, frequency in images is a rate of change. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.\n",
    "\n",
    "High-frequency components also correspond to the edges of objects in images, which can help us classify those objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fourier Transform\n",
    "\n",
    "![FT example](./images/1-3-2-3_FT_example.png)\n",
    "- An image of a soccer player and the corresponding frequency domain image (right). The concentrated points in the center of the frequency domain image mean that this image has a lot of low frequency (smooth background) components.\n",
    "\n",
    "\n",
    "The Fourier Transform (FT) is an important image processing tool which is used to decompose an image into its frequency components. The output of an FT represents the image in the frequency domain, while the input image is the spatial domain (x, y) equivalent. In the frequency domain image, each point represents a particular frequency contained in the spatial domain image. So, for images with a lot of high-frequency components (edges, corners, and stripes), there will be a number of points in the frequency domain at high frequency values.\n",
    "\n",
    "Take a look at how FT's are done with OpenCV, here(https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/1. Fourier Transform.ipynb\">1. Fourier Transform.ipynb</a> \n",
    "\n",
    "#### Additional reference for FT\n",
    "- https://plus.maths.org/content/fourier-transforms-images\n",
    "- https://darkpgmr.tistory.com/171"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Fourier Transform Quiz]\n",
    "\n",
    "Given the following image of diagonal stripes, what do you think it's Fourier transform image will look like?\n",
    "\n",
    "![FT Quiz](./images/1-3-2-4_FT_quiz.png)\n",
    "\n",
    "Image of left-leaning diagonal stripes of varying widths.\n",
    "\n",
    "[QUIZ QUESTION]For the input image of diagonal stripes, which of the options A-D, do you think is the most likely Fourier transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-3 : High-Pass Filters\n",
    "\n",
    "![what is hpf](./images/1-3-3-1_what_is_hpf.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![low frequency area in hpf](./images/1-3-3-2_low_frequency_area_in_hpf.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![high frequency area in hpf](./images/1-3-3-3_high_frequency_area_in_hpf.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edge\n",
    "\n",
    "![what is edge](./images/1-3-3-4_what_is_edge.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kernel\n",
    "\n",
    "![what is kernel](./images/1-3-3-5_what_is_kernel.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kernel\n",
    "\n",
    "![edge detection kernel](./images/1-3-3-6_edge_detection_kernel.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kernel_example](./images/1-3-3-7_kernel_example.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kernel_example](./images/1-3-3-8_kernel_example.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolution\n",
    "\n",
    "![what is convolution](./images/1-3-3-9_what_is_convolution.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![before convolution](./images/1-3-3-10_before_convolution.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![after convolution](./images/1-3-3-11_after_convolution.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![weighted sum in convolution](./images/1-3-3-12_weighted_sum_in_convolution.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edge handling in convolution processing\n",
    "\n",
    "![Edge handling in convolution processing](./images/1-3-3-13_Edge_handling_in_convolution_processing.png)\n",
    "- image ref : Udacity Computer Vision Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Extend : The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "\n",
    "- Padding : The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "- Crop : Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Kernels Quiz]\n",
    "\n",
    "Of the four kernels pictured above, which would be best for finding and enhancing horizontal edges and lines in an image?\n",
    "\n",
    "![Kernel Quiz](./images/1-3-3-14_Kernel_quiz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-4 : Image Gradients and Sobel Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image Gradients\n",
    "\n",
    "![Kernel Quiz](./images/1-3-4-1_Image_Gradients.png)\n",
    "\n",
    "Gradients are a measure of intensity change in an image, and they generally mark object boundaries and changing area of light and dark. If we think back to treating images as functions, F(x, y), we can think of the gradient as a derivative operation F’(x, y). Where the derivative is a measurement of intensity change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sobel filters\n",
    "\n",
    "![Sobel filters](./images/1-3-4-2_Sobel_filters.png)\n",
    "\n",
    "The Sobel filter is very commonly used in edge detection and in finding patterns in intensity in an image. Applying a Sobel filter to an image is a way of taking (an approximation) of the derivative of the image in the x or y direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Sobel filters applied image](./images/1-3-4-3_Sobel_filters_applied_image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Magnitude of image gradients\n",
    "\n",
    "![Magnitude of image gradients](./images/1-3-4-4_Magnitude_of_image_gradients.png)\n",
    "\n",
    "Sobel also detects which edges are strongest. This is encapsulated by the magnitude of the gradient; the greater the magnitude, the stronger the edge is. The magnitude, or absolute value, of the gradient is just the square root of the squares of the individual x and y gradients. For a gradient in both the x and y directions, the magnitude is the square root of the sum of the squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Direction of image gradients\n",
    "\n",
    "![Direction of image gradients](./images/1-3-4-5_Direction_of_image_gradients.png)\n",
    "\n",
    "In many cases, it will be useful to look for edges in a particular orientation. For example, we may want to find lines that only angle upwards or point left. By calculating the direction of the image gradient in the x and y directions separately, we can determine the direction of that gradient!\n",
    "\n",
    "The direction of the gradient is simply the inverse tangent (arctangent) of the y gradient divided by the x gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/2. Finding Edges and Custom Kernels.ipynb\">2. Finding Edges and Custom Kernels.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-5 : Low-Pass Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Noise](./images/1-3-5-1_Noise.png)\n",
    "\n",
    "noise is generally seen as speckles or discoloration in an image and it doesn't contain any useful information.\n",
    "It might even mess with processing steps such as an edge detection when high pass filters can amplify noise if it's not removed first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Romove noise by Low-Pass Filter\n",
    "\n",
    "![Romove noise by Low-Pass Filter](./images/1-3-5-2_Romove_noise_by_LPF.png)\n",
    "\n",
    "The most common way to remove noise is by using a low pass filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Low-Pass Filter](./images/1-3-5-3_LPF.png)\n",
    "\n",
    "These filters block certain high frequency content and effectively blur or smooth the appearance of an image, and this reduces high frequency noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Averaging Filter\n",
    "\n",
    "![Averaging Filter](./images/1-3-5-4_AF.png)\n",
    "\n",
    "These filters block certain high frequency content and effectively blur or smooth the appearance of an image, and this reduces high frequency noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![calculation of Averaging Filter](./images/1-3-5-5_calculation_of_AF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![convolution of Averaging Filter](./images/1-3-5-6_convolution_of_AF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gaussian Blur = Weighted Averaging Filter\n",
    "\n",
    "![Gaussian Blur](./images/1-3-5-7_Gaussian_Blur.png)\n",
    "\n",
    "Read more about the math behind a Gaussian blur, here(https://en.wikipedia.org/wiki/Gaussian_blur); this also shows a few more uses for blur in images.\n",
    "\n",
    "The OpenCV function GaussianBlur is documented on this page(http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Weighted Averaging Filter](./images/1-3-5-8_Weighted_Averaging_Filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/3. Gaussian Blur.ipynb\">3. Gaussian Blur.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/4. Fourier Transform of Filters.ipynb\">4. Fourier Transform of Filters.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-6 : Canny Edge Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Edge Detection](./images/1-3-6-1_Edge_Detection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Canny Edge Detection step1](./images/1-3-6-2_Canny_Edge_Detection_step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Canny Edge Detection step2](./images/1-3-6-3_Canny_Edge_Detection_step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Canny Edge Detection step3](./images/1-3-6-4_Canny_Edge_Detection_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Canny Edge Detection step4](./images/1-3-6-5_Canny_Edge_Detection_step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Canny Edge Detection Hysteresis](./images/1-3-6-6_Hysteresis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Canny Edge Detection Hysteresis explanation](./images/1-3-6-7_Hysteresis_explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/5. Canny Edge Detection.ipynb\">5. Canny Edge Detection.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-7 : Shape Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edge Detection\n",
    "\n",
    "Now that you've seen how to define and use image filters for smoothing images and detecting the edges (high-frequency) components of objects in an image, let's move one step further. The next few videos will be all about how we can use what we know about pattern recognition in images to begin identifying unique shapes and then objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edges to Boundaries and Shapes\n",
    "\n",
    "![Shape Detection by Hough Transform](./images/1-3-7-1_shape_detection_by_Hough_Transform.png)\n",
    "\n",
    "The Hough transform is used in a variety of shape-recognition applications, as seen in the images pictured below. On the left you see how a Hough transform can find the edges of a phone screen and on the right you see how it's applied to an aerial image of farms (green circles in this image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-8 : Hough Transform\n",
    "\n",
    "#### Additional Reference\n",
    "- https://wkdtjsgur100.github.io/Hough-Transform/\n",
    "- http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step1](./images/1-3-8-1_Hough_Transform_step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step2](./images/1-3-8-2_Hough_Transform_step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step3](./images/1-3-8-3_Hough_Transform_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step4](./images/1-3-8-4_Hough_Transform_step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step5](./images/1-3-8-5_Hough_Transform_step5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step6](./images/1-3-8-6_Hough_Transform_step6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step7](./images/1-3-8-7_Hough_Transform_step7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step8](./images/1-3-8-8_Hough_Transform_step8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Hough Transform step9](./images/1-3-8-9_Hough_Transform_step9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Hough Space Quiz]\n",
    "\n",
    "[QUIZ QUESTION] What will this image look like in Hough space? Choose the correct plot.\n",
    "\n",
    "![Hough Space Quiz](./images/1-3-8-10_Hough_Space_Quiz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/6_1. Hough lines.ipynb\">6_1. Hough lines.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/6_2. Hough circles, agriculture.ipynb\">6_2. Hough circles, agriculture.ipynb</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-3-9 : Object Recognition & Haar Cascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Extraction and Object Recognition\n",
    "\n",
    "![feature extraction before training data](./images/1-3-9-1_feature-extraction_before_training_data.png)\n",
    "\n",
    "Extracting features and patterns in image data, using things like image filters, is the basis for many object recognition techniques. In the image below, we see a classification pipeline that is looking at an image of a banana; the image first goes through some filters and processing steps to form a feature that represent that banana, and this is used to help classify it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Haar Cascade and Face Recognition\n",
    "\n",
    "we'll see how we can use a feature-based classifier to do face recognition.\n",
    "\n",
    "The method we'll be looking at is called a Haar cascade classifier. It's a machine learning based approach where a cascade function is trained to solve a binary classification problem: face or not-face; it trains on a lot of positive (face) and negative (not-face) images, as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Haar Cascade\n",
    "\n",
    "![Haar Cascade step1](./images/1-3-9-2_Haar_Cascade_step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Haar Cascade step2](./images/1-3-9-3_Haar_Cascade_step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Haar Cascade step3](./images/1-3-9-4_Haar_Cascade_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Haar Cascade step4](./images/1-3-9-5_Haar_Cascade_step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Haar Cascade step5](./images/1-3-9-6_Haar_Cascade_step5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Haar Cascade step6](./images/1-3-9-7_Haar_Cascade_step6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Haar Cascade step7](./images/1-3-9-8_Haar_Cascade_step7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hands-On :  <a href=\"../1_2_Convolutional_Filters_Edge_Detection/7. Haar Cascade, Face Detection.ipynb\">7. Haar Cascade, Face Detection.ipynb</a> "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cv-nd",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
